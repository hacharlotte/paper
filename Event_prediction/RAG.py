
def build_causal_event_sentences(events, sentences, causal_pairs):
    # ç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼šå› æœå¯¹ä¸­å‡ºç°çš„äº‹ä»¶ï¼ˆä»¥å…ƒç»„æ–¹å¼æ ‡è¯†ï¼‰
    def event_key(event):
        return (event["s"], event["v"], event["o"], event["p"])

    causal_event_set = set()
    for pair in causal_pairs:
        causal_event_set.add(event_key(pair["e1"]))
        causal_event_set.add(event_key(pair["e2"]))

    # æ„å»ºå¥å­ + offsets + eventsï¼ˆä»…é™å› æœäº‹ä»¶ï¼‰
    result = []
    for sid, tokens in enumerate(sentences):
        text = ""
        offsets = []
        for i, tok in enumerate(tokens):
            if i > 0:
                text += " "
            start = len(text)
            text += tok
            end = len(text)
            offsets.append((start, end))

        # æ”¶é›†å½“å‰å¥å­çš„æ‰€æœ‰å› æœäº‹ä»¶
        event_list = []
        for e in events:
            if e["sentence_id"] != sid:
                continue
            key = event_key(e["event"])
            if key in causal_event_set:
                trig_idx = e["trigger_index"]
                start, end = offsets[trig_idx]
                new_event = {
                    **e["event"],
                    "sentence_id": sid,
                    "trigger_index": trig_idx,
                    "from": start,
                    "to": end
                }
                event_list.append(new_event)

        if event_list:
            result.append({
                "text": text,
                "events": event_list
            })

    return result


def attach_sentiment_to_filtered_events(output_json, data):
    sentiment_map = {0: "positive", 1: "negative", 2: "neutral"}

    # è®¡ç®—é¢„æµ‹æ•°æ˜¯å¦ä¸eventsæ•°ä¸€è‡´
    total_events = sum(len(entry["events"]) for entry in output_json)
    assert total_events == len(data), f"é¢„æµ‹æ•°é‡({len(data)})ä¸äº‹ä»¶æ•°é‡({total_events})ä¸ä¸€è‡´"

    idx = 0
    for entry in output_json:
        for event in entry["events"]:
            event["sentiment"] = sentiment_map.get(data[idx], "unknown")
            idx += 1

    return output_json

def sync_sentiment_back_to_events(events, result, data):
    # Step 1: æ„å»º result ä¸­çš„äº‹ä»¶åˆ° sentiment çš„æ˜ å°„
    sentiment_map = {0: "positive", 1: "negative", 2: "neutral"}
    event_to_sentiment = {}
    idx = 0

    for item in result:
        for ev in item["events"]:
            key = (
                ev["s"], ev["v"], ev["o"], ev["p"],
                ev["sentence_id"], ev["trigger_index"]
            )
            event_to_sentiment[key] = sentiment_map.get(data[idx], "unknown")
            idx += 1

    # Step 2: å°† sentiment åŒæ­¥å†™å›åŸå§‹ events ä¸­
    for e in events:
        ev = e["event"]
        key = (
            ev["s"], ev["v"], ev["o"], ev["p"],
            e["sentence_id"], e["trigger_index"]
        )
        if key in event_to_sentiment:
            ev["sentiment"] = event_to_sentiment[key]
        else:
            ev["sentiment"] = None  # ä¸æ˜¯å› æœäº‹ä»¶å¯¹ä¸­çš„ï¼Œè®¾ä¸º None æˆ–ä¸è®¾

    return events

def build_event_sentences(events, sentences):
    sentence_infos = []
    for tokens in sentences:
        text = ""
        offsets = []
        for i, tok in enumerate(tokens):
            if i > 0:
                text += " "
            start = len(text)
            text += tok
            end = len(text)
            offsets.append((start, end))
        sentence_infos.append({"sentence": text, "offsets": offsets, "events": []})

    # æ’å…¥äº‹ä»¶å¹¶æ·»åŠ å‡†ç¡®çš„ from/to ç´¢å¼•
    for e in events:
        sid = e["sentence_id"]
        trig_idx = e["trigger_index"]
        event_data = e["event"]
        start, end = sentence_infos[sid]["offsets"][trig_idx]
        new_event = {
            **event_data,
            "sentence_id": sid,
            "trigger_index": trig_idx,
            "from": start,
            "to": end
        }
        sentence_infos[sid]["events"].append(new_event)

    # è¾“å‡ºæœ€ç»ˆç»“æœç»“æ„
    result = [
        {
            "text": info["sentence"],
            "events": info["events"]
        }
        for info in sentence_infos if info["events"]
    ]
import sqlite3

def check_tables(db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    # æŸ¥è¯¢æ‰€æœ‰è¡¨å
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = cursor.fetchall()
    print("Tables in database:", tables)
    conn.close()


def check_schema(db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # æ£€æŸ¥ Relations è¡¨çš„å­—æ®µ
    cursor.execute("PRAGMA table_info(Relations)")
    rel_columns = cursor.fetchall()
    print("Relations è¡¨å­—æ®µ:", [col for col in rel_columns])  # è¾“å‡ºå­—æ®µå

    # æ£€æŸ¥ Eventualities è¡¨çš„å­—æ®µ
    cursor.execute("PRAGMA table_info(Eventualities)")
    ev_columns = cursor.fetchall()
    print("Eventualities è¡¨å­—æ®µ:", [col for col in ev_columns])

    conn.close()


import sqlite3

import sqlite3

def extract_aser_causal_triples(
    db_path,
    output_path="aser_causal_triples.txt",
    prob_threshold=0.7
):
    """
    ä» ASER SQLite ä¸­æå–å› æœç›¸å…³çš„ä¸‰å…ƒç»„ï¼šReason, Result, Condition
    """
    causal_relations = ['Reason', 'Result', 'Condition']

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # è¯»å–æ‰€æœ‰äº‹ä»¶
    cursor.execute("SELECT _id, words FROM Eventualities")

    id2event = {row[0]: row[1] for row in cursor.fetchall() if row[1]}

    # è¯»å–æ‰€æœ‰å…³ç³»è¡Œ
    cursor.execute("SELECT * FROM Relations")
    rows = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]

    print(f"ğŸ” å…±è¯»å– Relations è¡¨ä¸­çš„ {len(rows)} æ¡è®°å½•")

    triples = []
    for row in rows:
        row_dict = dict(zip(columns, row))
        e1_id = row_dict["event1_id"]
        e2_id = row_dict["event2_id"]

        if e1_id not in id2event or e2_id not in id2event:
            continue

        e1_text = id2event[e1_id].strip()
        e2_text = id2event[e2_id].strip()

        for rel in causal_relations:
            prob = row_dict.get(rel)
            if prob is not None and prob >= prob_threshold:
                triple = f"{e1_text} â†’ {rel.lower()} â†’ {e2_text}"
                triples.append(triple)

    print(f"âœ… æå–å› æœä¸‰å…ƒç»„ï¼š{len(triples)} æ¡ï¼ˆprob â‰¥ {prob_threshold}ï¼‰")

    with open(output_path, "w", encoding="utf-8") as f:
        for t in triples:
            f.write(t + "\n")

    print(f"ğŸ‰ å·²ä¿å­˜è‡³ {output_path}")

    conn.close()


from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from tqdm import tqdm
import os
from app.utils.retriever import AserRetriever
from app.utils.reranker import TripleReranker
def load_model(model_path_or_name):
    if os.path.exists(model_path_or_name):
        print("ğŸ“¦ æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹...")
        return SentenceTransformer(model_path_or_name)
    else:
        print("ğŸŒ æ­£åœ¨ä» HuggingFace ä¸‹è½½æ¨¡å‹...")
        return SentenceTransformer(model_path_or_name)

def build_faiss_index_for_CT(
    triple_path="causal_event_pairs.txt",
    model_name=r"D:\NLP\project_for_system\my_backend\app\bge-small-en-v1.5",  # å¯æ›¿æ¢ä¸ºä½ è‡ªå·±çš„
    index_save_path="faiss_index/ct_causal.index",
    text_save_path="faiss_index/ct_causal_texts.npy",
    normalize=True
):
    """
    æ„å»ºä¸‰å…ƒç»„å‘é‡ç´¢å¼•åº“ï¼ˆç”¨äºRAGè¯­ä¹‰æ£€ç´¢ï¼‰
    """
    # åŠ è½½ä¸‰å…ƒç»„
    with open(triple_path, 'r', encoding='utf-8') as f:
        triples = [line.strip() for line in f if line.strip()]

    print(f"ğŸ“„ å…±åŠ è½½ä¸‰å…ƒç»„: {len(triples)} æ¡")

    # åŠ è½½æ¨¡å‹
    print(f"ğŸ§  åŠ è½½ç¼–ç æ¨¡å‹: {model_name}")
    model = SentenceTransformer(model_name)

    # å‘é‡ç¼–ç 
    print("ğŸ” æ­£åœ¨ç¼–ç ä¸‰å…ƒç»„...")
    embeddings = model.encode(triples, show_progress_bar=True, normalize_embeddings=normalize)
    embeddings = np.array(embeddings).astype("float32")

    # æ„å»º FAISS ç´¢å¼•
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim) if normalize else faiss.IndexFlatL2(dim)
    index.add(embeddings)
    print(f"âœ… æ„å»ºå®Œæ¯•ï¼ç´¢å¼•å‘é‡æ•°: {index.ntotal}")

    # ä¿å­˜
    import os
    os.makedirs("faiss_index", exist_ok=True)
    faiss.write_index(index, index_save_path)
    np.save(text_save_path, np.array(triples))

    print(f"ğŸ“¦ ç´¢å¼•ä¿å­˜è‡³: {index_save_path}")
    print(f"ğŸ“¦ åŸæ–‡ä¿å­˜è‡³: {text_save_path}")
def build_faiss_index(
    triple_path="aser_triples.txt",
    model_name=r"D:\NLP\project_for_system\my_backend\app\bge-small-en-v1.5",  # å¯æ›¿æ¢ä¸ºä½ è‡ªå·±çš„
    index_save_path="faiss_index/aser_causal.index",
    text_save_path="faiss_index/aser_causal_texts.npy",
    normalize=True
):
    """
    æ„å»ºä¸‰å…ƒç»„å‘é‡ç´¢å¼•åº“ï¼ˆç”¨äºRAGè¯­ä¹‰æ£€ç´¢ï¼‰
    """
    # åŠ è½½ä¸‰å…ƒç»„
    with open(triple_path, 'r', encoding='utf-8') as f:
        triples = [line.strip() for line in f if line.strip()]

    print(f"ğŸ“„ å…±åŠ è½½ä¸‰å…ƒç»„: {len(triples)} æ¡")

    # åŠ è½½æ¨¡å‹
    print(f"ğŸ§  åŠ è½½ç¼–ç æ¨¡å‹: {model_name}")
    model = SentenceTransformer(model_name)

    # å‘é‡ç¼–ç 
    print("ğŸ” æ­£åœ¨ç¼–ç ä¸‰å…ƒç»„...")
    embeddings = model.encode(triples, show_progress_bar=True, normalize_embeddings=normalize)
    embeddings = np.array(embeddings).astype("float32")

    # æ„å»º FAISS ç´¢å¼•
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim) if normalize else faiss.IndexFlatL2(dim)
    index.add(embeddings)
    print(f"âœ… æ„å»ºå®Œæ¯•ï¼ç´¢å¼•å‘é‡æ•°: {index.ntotal}")

    # ä¿å­˜
    import os
    os.makedirs("faiss_index", exist_ok=True)
    faiss.write_index(index, index_save_path)
    np.save(text_save_path, np.array(triples))

    print(f"ğŸ“¦ ç´¢å¼•ä¿å­˜è‡³: {index_save_path}")
    print(f"ğŸ“¦ åŸæ–‡ä¿å­˜è‡³: {text_save_path}")
import re
import networkx as nx
from typing import List
from sentence_transformers import SentenceTransformer
def is_bad_event(text: str):
    stopwords = {"be", "do", "know", "say", "have", "make", "see"}
    tokens = text.lower().split()
    return (
        len(tokens) <= 2 or
        any(token in stopwords for token in tokens) or
        re.match(r"^\s*(he|she|it|they|i|you)\s*$", text.strip().lower())
    )

def filter_bad_triples(triples):
    filtered = []
    for t in triples:
        try:
            e1, rel, e2 = t.split("â†’")
            if not is_bad_event(e1) and not is_bad_event(e2):
                filtered.append(t.strip())
        except:
            continue
    return filtered

def load_graph_from_triples(triple_path: str) -> nx.DiGraph:
    """
    æ ¹æ®ä¸‰å…ƒç»„æ–‡ä»¶æ„å»ºäº‹ä»¶å› æœæœ‰å‘å›¾ G(event1 â†’ event2)
    æ¯æ¡è¾¹ä¿å­˜äº†å› æœå…³ç³» label
    """
    G = nx.DiGraph()
    with open(triple_path, 'r', encoding='utf-8') as f:
        for line in f:
            if "â†’" not in line:
                continue
            try:
                e1, rel, e2 = [s.strip() for s in line.split("â†’")]
                G.add_edge(e1, e2, label=rel)
            except Exception as e:
                print(f"[WARN] Parse failed: {line.strip()} | {e}")
    return G


def semantic_path_beam_search(
    G: nx.DiGraph,
    start_nodes: List[str],
    query: str,
    encoder: SentenceTransformer,
    max_hops: int = 5,
    beam_width: int = 5,
    top_k_paths: int = 10,
    sim_threshold: float = 0.3
) -> List[List[str]]:
    """
    ä½¿ç”¨ Beam Search è¿›è¡Œå¤šè·³è¯­ä¹‰è·¯å¾„æœç´¢
    """
    query_emb = encoder.encode([query], normalize_embeddings=True)[0]
    path_candidates = []

    def path_score(path_embs):
        return np.mean([np.dot(e, query_emb) for e in path_embs])

    for start in start_nodes:
        if start not in G:
            continue

        # åˆå§‹åŒ– beam ä¸º [[start]] è·¯å¾„åˆ—è¡¨
        beam = [[start]]

        for _ in range(max_hops):
            new_beam = []
            for path in beam:
                last_node = path[-1]
                for succ in G.successors(last_node):
                    if succ in path:
                        continue  # é¿å…ç¯
                    new_path = path + [succ]
                    embs = encoder.encode(new_path, normalize_embeddings=True)
                    score = path_score(embs)
                    if score >= sim_threshold:
                        new_beam.append((new_path, score))
            if not new_beam:
                break
            # ä»…ä¿ç•™å‰ beam_width æ¡è·¯å¾„
            new_beam.sort(key=lambda x: x[1], reverse=True)
            beam = [p for p, _ in new_beam[:beam_width]]

        # æ·»åŠ æ‰€æœ‰ beam ä¸­çš„è·¯å¾„
        path_candidates.extend(beam)

    # å…¨å±€ç­›é€‰ top_k_paths
    scored_paths = [
        (path, path_score(encoder.encode(path, normalize_embeddings=True)))
        for path in path_candidates
    ]
    scored_paths.sort(key=lambda x: x[1], reverse=True)
    top_paths = [p for p, _ in scored_paths[:top_k_paths]]
    return top_paths


def deduplicate_and_filter_paths(
    paths: List[List[str]],
    encoder: SentenceTransformer,
    min_len: int = 3,
    min_diversity: float = 0.2
) -> List[List[str]]:
    """
    è·¯å¾„å»é‡ + ç®€å•çš„å¤šæ ·æ€§è¿‡æ»¤
    """
    seen = set()
    good_paths = []

    def diversity_score(path_embs):
        return np.mean([1 - np.dot(path_embs[i], path_embs[i+1]) for i in range(len(path_embs)-1)])

    for path in paths:
        if len(path) < min_len:
            continue
        t = tuple(path)
        if t in seen:
            continue
        seen.add(t)
        embs = encoder.encode(path, normalize_embeddings=True)
        if diversity_score(embs) >= min_diversity:
            good_paths.append(path)
    return good_paths


def print_paths(paths: List[List[str]], encoder: SentenceTransformer, query: str):
    """
    æ‰“å°è·¯å¾„åŠå…¶ä¸ query çš„å¹³å‡ç›¸ä¼¼åº¦
    """
    query_emb = encoder.encode([query], normalize_embeddings=True)[0]
    for path in paths:
        embs = encoder.encode(path, normalize_embeddings=True)
        score = np.mean([np.dot(e, query_emb) for e in embs])
        print(" â†’ ".join(path), f"(score={score:.3f})")

# build_faiss_index_for_CT()
# # ç¤ºä¾‹å…¥å£
# if __name__ == '__main__':
#     from sentence_transformers import SentenceTransformer
#
#     # é…ç½®è·¯å¾„
#     TRIPLE_PATH = "aser_triples.txt"
#     MODEL_PATH = r"D:\NLP\project_for_system\my_backend\app\bge-small-en-v1.5"
#
#     # åŠ è½½å›¾è°±å’Œæ¨¡å‹
#     G = load_graph_from_triples(TRIPLE_PATH)
#
#     # è¾“å…¥äº‹ä»¶ä¸Šä¸‹æ–‡
#     context_events = [
#         "friends knew simpson",
#         "simpson became famous",
#         "simpson was convicted",
#         "law applied to simpson"
#     ]
#     query = ". ".join(context_events)
#     retriever = AserRetriever(
#         index_path="faiss_index/aser_causal.index",
#         text_path="faiss_index/aser_causal_texts.npy",
#         model_name=r"D:\NLP\project_for_system\my_backend\app\bge-small-en-v1.5"  # æˆ–ä½ è‡ªå·±çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
#     )
#
#     context_events = [
#         "friends knew simpson",
#         "simpson became famous",
#         "simpson was convicted",
#         "law applied to simpson"
#     ]
#     index = faiss.read_index("faiss_index/aser_causal.index")
#     texts = np.load("faiss_index/aser_causal_texts.npy", allow_pickle=True).tolist()
#     # 1. åŠ è½½ ASER å…¨å›¾ï¼ˆå¤šè·³ç»“æ„æ”¯æŒï¼‰
#     graph = load_graph_from_triples("aser_triples.txt")
#
#     # 1. åŸå§‹æ£€ç´¢
#     retrieved,encoder = retriever.retrieve(context_events, top_k=20)
#
#     # # 2. è¿‡æ»¤ä½è´¨é‡äº‹ä»¶
#     # cleaned = filter_bad_triples(retrieved)
#
#     # 3. ä½¿ç”¨ reranker è¿›è¡Œè¯­ä¹‰æ’åº
#     reranker = TripleReranker()
#     top_reranked = reranker.rerank(query, retrieved, top_k=5)
#     seed_events = set()
#     for triple in top_reranked:
#         e1, _, e2 = [s.strip() for s in triple.split("â†’")]
#         seed_events.add(e1)
#         seed_events.add(e2)
#     # æ‰§è¡Œè·¯å¾„æœç´¢
#     paths = semantic_path_beam_search(
#         G=G,
#         start_nodes=seed_events,
#         query=query,
#         encoder=encoder,
#         max_hops=5,
#         beam_width=5,
#         top_k_paths=20,
#         sim_threshold=0.3
#     )
#
#     # è·¯å¾„è¿‡æ»¤
#     cleaned = deduplicate_and_filter_paths(paths, encoder, min_len=2, min_diversity=0.2)
#
#     print("Top semantic paths:")
#     print_paths(cleaned, encoder, query)

def RAG(context_events):
    retriever = AserRetriever(
        index_path=r"D:\NLP\project_for_system\my_backend\app\utils\faiss_index\ct_causal.index",
        text_path=r"D:\NLP\project_for_system\my_backend\app\utils\faiss_index\ct_causal_texts.npy",
        model_name=r"D:\NLP\project_for_system\my_backend\app\bge-small-en-v1.5"  # æˆ–ä½ è‡ªå·±çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
    )
    query = ". ".join(context_events)

    # 1. åŸå§‹æ£€ç´¢
    retrieved, encoder = retriever.retrieve(context_events, top_k=50)


    # 2. ä½¿ç”¨ reranker è¿›è¡Œè¯­ä¹‰æ’åº
    reranker = TripleReranker()
    top_reranked = reranker.rerank(query, retrieved, top_k=8)
    return top_reranked
# if __name__ == '__main__':
#     retriever = AserRetriever(
#         index_path=r"D:\NLP\project_for_system\my_backend\app\utils\faiss_index\aser_causal.index",
#         text_path=r"D:\NLP\project_for_system\my_backend\app\utils\faiss_index\aser_causal_texts.npy",
#         model_name=r"D:\NLP\project_for_system\my_backend\app\bge-small-en-v1.5"  # æˆ–ä½ è‡ªå·±çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
#     )
#     context_events = ["trump"]
#     query = ". ".join(context_events)
#
#     # 1. åŸå§‹æ£€ç´¢
#     retrieved, encoder = retriever.retrieve(context_events, top_k=50)
#
#     # # 2. è¿‡æ»¤ä½è´¨é‡äº‹ä»¶
#     # cleaned = filter_bad_triples(retrieved)
#
#     # 3. ä½¿ç”¨ reranker è¿›è¡Œè¯­ä¹‰æ’åº
#     reranker = TripleReranker()
#     top_reranked = reranker.rerank(query, retrieved, top_k=8)

#
#     # check_tables('../output/KG.db')
#     # check_schema('../output/KG.db')
#     # extract_aser_causal_triples('../output/KG.db', "aser_triples.txt", prob_threshold=0.75)
#     # model = load_model(r"D:\NLP\project_for_system\my_backend\app\bge-small-en-v1.5")
#
#     # build_faiss_index()
#     retriever = AserRetriever(
#         index_path="faiss_index/aser_causal.index",
#         text_path="faiss_index/aser_causal_texts.npy",
#         model_name=r"D:\NLP\project_for_system\my_backend\app\bge-small-en-v1.5"  # æˆ–ä½ è‡ªå·±çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
#     )
#
#     context_events = [
#         "friends knew simpson",
#         "simpson became famous",
#         "simpson was convicted",
#         "law applied to simpson"
#     ]
#     index = faiss.read_index("faiss_index/aser_causal.index")
#     texts = np.load("faiss_index/aser_causal_texts.npy", allow_pickle=True).tolist()
#     # 1. åŠ è½½ ASER å…¨å›¾ï¼ˆå¤šè·³ç»“æ„æ”¯æŒï¼‰
#     graph = load_graph_from_triples("aser_triples.txt")
#     query = ". ".join(context_events)
#
#     # 1. åŸå§‹æ£€ç´¢
#     retrieved,encoder = retriever.retrieve(context_events, top_k=50)
#
#     # # 2. è¿‡æ»¤ä½è´¨é‡äº‹ä»¶
#     # cleaned = filter_bad_triples(retrieved)
#
#     # 3. ä½¿ç”¨ reranker è¿›è¡Œè¯­ä¹‰æ’åº
#     reranker = TripleReranker()
#     top_reranked = reranker.rerank(query, retrieved, top_k=8)
#     print(top_reranked)
#     seed_events = set()
#     for triple in top_reranked:
#         e1, _, e2 = [s.strip() for s in triple.split("â†’")]
#         seed_events.add(e1)
#         seed_events.add(e2)
#     from itertools import chain
#
#     paths = semantic_random_walk_paths_from_global_index_fixed(
#         G=graph,
#         start_nodes=seed_events,
#         query=query,
#         encoder=encoder,
#         faiss_index=index,
#         faiss_texts=texts,
#         max_hops=5,
#         walks_per_node=10,
#         top_k_per_hop=10,
#         sim_threshold=0.2
#     )
#
#     cleaned_paths = deduplicate_and_filter_paths(
#         paths=paths,
#         encoder=encoder,
#         min_len=3,
#         min_diversity=0.2
#     )
#
#     print("éšæœºæ¸¸èµ°ç”Ÿæˆçš„è·¯å¾„æ•°é‡:", len(paths))
#     for p in paths[:5]:
#         print(" -> ".join(p))


